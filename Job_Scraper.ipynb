{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "import pandas as pd\n",
    "import os, re, time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header= {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) ' \n",
    "      'AppleWebKit/537.11 (KHTML, like Gecko) '\n",
    "      'Chrome/23.0.1271.64 Safari/537.11',\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "##      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "##      'Accept-Encoding': 'none',\n",
    "##      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping from Indeed and Jobstreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "##===============================================Function for Indeed==============================================##\n",
    "\n",
    "def load_indeed_jobs_div(job_title, location):\n",
    "    getVars = {'q': job_title, 'l': location, 'fromage': 'last', 'sort': 'date', 'limit': 100}\n",
    "    url = ('https://sg.indeed.com/jobs?'+ urllib.parse.urlencode(getVars))\n",
    "    page = requests.get(url,verify=False)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    job_soup = soup.find(id='resultsCol')\n",
    "    return job_soup\n",
    "\n",
    "def extract_job_information_indeed(job_soup,desired_characs):\n",
    "    job_elems = job_soup.find_all('div',class_='jobsearch-SerpJobCard')\n",
    "    \n",
    "    cols = []\n",
    "    extracted_info = []\n",
    "    \n",
    "    if 'titles' in desired_characs:\n",
    "        titles = []\n",
    "        cols.append('titles')\n",
    "        for job_elem in job_elems:\n",
    "            titles.append(extract_job_title_indeed(job_elem))\n",
    "        extracted_info.append(titles)\n",
    "\n",
    "    if 'companies' in desired_characs:\n",
    "        companies = []\n",
    "        cols.append('companies')\n",
    "        for job_elem in job_elems:\n",
    "            companies.append(extract_company_indeed(job_elem))\n",
    "        extracted_info.append(companies)\n",
    "    \n",
    "    if 'links' in desired_characs:\n",
    "        links = []\n",
    "        cols.append('links')\n",
    "        for job_elem in job_elems:\n",
    "            links.append(extract_link_indeed(job_elem))\n",
    "        extracted_info.append(links)\n",
    "\n",
    "    if 'date_listed' in desired_characs:\n",
    "        dates = []\n",
    "        cols.append('date_listed')\n",
    "        for job_elem in job_elems:\n",
    "            dates.append(extract_date_indeed(job_elem))\n",
    "        extracted_info.append(dates)\n",
    "        \n",
    "    if 'salary' in desired_characs:\n",
    "        salary = []\n",
    "        cols.append('salary')\n",
    "        for job_elem in job_elems:\n",
    "            salary.append(extract_salary_indeed(job_elem))\n",
    "        extracted_info.append(salary)\n",
    "    \n",
    "    jobs_list = {}\n",
    "    \n",
    "    for j in range(len(cols)):\n",
    "        jobs_list[cols[j]]=extracted_info[j]\n",
    "        \n",
    "    num_listings = len(extracted_info[0])\n",
    "    \n",
    "    return jobs_list, num_listings\n",
    "\n",
    "\n",
    "\n",
    "def extract_job_title_indeed(job_elem):\n",
    "    title_elem = job_elem.find('h2',class_='title')\n",
    "    title = title_elem.text.strip().replace('\\nnew','')\n",
    "    return title\n",
    "\n",
    "def extract_company_indeed(job_elem):\n",
    "    company_elem = job_elem.find('span',class_='company')\n",
    "    company = company_elem.text.strip()\n",
    "    return company\n",
    "\n",
    "def extract_link_indeed(job_elem):\n",
    "    link = job_elem.find('a',class_='jobtitle')['href']\n",
    "    link = 'sg.indeed.com' + link\n",
    "    return link\n",
    "\n",
    "def extract_date_indeed(job_elem):\n",
    "    date_elem = job_elem.find('span',class_='date')\n",
    "    date = date_elem.text.strip()\n",
    "    return date\n",
    "\n",
    "def extract_salary_indeed(job_elem):\n",
    "    try:\n",
    "        salary_elem = job_elem.find('span',class_='salary')\n",
    "        salary = salary_elem.text.strip()\n",
    "    except:\n",
    "        salary='NA'\n",
    "    return salary\n",
    "\n",
    "\n",
    "##===============================================Function for JobStreet==============================================##\n",
    "\n",
    "\"\"\" get all position <a> tags for a single job role, triggered by linksByRoles function, results stored in a list \"\"\"    \n",
    "\n",
    "def linksbyKey(key):\n",
    "    base_url = 'https://www.jobstreet.com.sg/en/job-search/job-vacancy.php?'\n",
    "    pay_load = {'key':'','area':1,'option':1,'pg':None,'classified':1,'src':16,'srcr':12}\n",
    "    pay_load['key'] = key\n",
    "    \n",
    "    pn = 1\n",
    "    \n",
    "    position_links = []\n",
    "    loaded = True\n",
    "    while loaded:\n",
    "        print('Loading page {}...'.format(pn))\n",
    "        pay_load['pg'] = pn\n",
    "        url =base_url+ urllib.parse.urlencode(pay_load)\n",
    "        page = requests.get(url,headers=header,verify=False)\n",
    "        \n",
    "        soup = BeautifulSoup(page.text,'html.parser')\n",
    "        links = soup.find_all('a',class_=\"DvvsL_0 _1p9OP\",href=True)\n",
    "        \n",
    "        if not len(links):\n",
    "            loaded = False\n",
    "        else:\n",
    "            position_links += links\n",
    "            pn +=1\n",
    "    return position_links\n",
    "        \n",
    "\n",
    "\"\"\" get all position links for the list of roles, results stored in a dict\"\"\"   \n",
    "    \n",
    "def linksByRoles(roles):\n",
    "    ## roles: a list of job roles\n",
    "    ## return: a dictionary of links\n",
    "\n",
    "    links_dic = dict()\n",
    "    # scrape key words one by one\n",
    "    for role in roles:\n",
    "        print('Scraping position: ', role, ' ...')\n",
    "        links_dic[role] = linksbyKey(role)\n",
    "        print('{} {} positions found!'.format(len(links_dic[role]),role))\n",
    "    return links_dic\n",
    "\n",
    "\n",
    "\"\"\" parse HTML strings for the list of roles\"\"\"\n",
    "    \n",
    "def parseLinks(links_dic):\n",
    "    ## links_dic: a dictionary of links\n",
    "    ## return: print parsed results to .csv file\n",
    "\n",
    "    for key in links_dic:\n",
    "        \n",
    "        jobs = []\n",
    "        for link in links_dic[key]:\n",
    "            jobs.append([key] + getJobDetail(link))\n",
    "\n",
    "        # transfrom the result to a pandas.DataFrame\n",
    "        result = pd.DataFrame(jobs,columns=['key_word','company_name','job_title','company_industry','qualification','location','salary','date_posted'])\n",
    "\n",
    "        # add a column denoting if the position is posted by a recuriter company\n",
    "        result['postedByHR'] = result.company_industry.apply(lambda x:True if x and x.find('Human Resources')>-1 else False)\n",
    "        num_listings = len(result)\n",
    "        # save result,\n",
    "#        file_name = key+'.csv'\n",
    "#        result.to_csv(file_name,index=False)\n",
    "        return result, num_listings\n",
    "    \n",
    "\n",
    "\"\"\" extract details from post detail page \"\"\"\n",
    "def getJobDetail(job_href):\n",
    "    ## job_href: a post url\n",
    "    ## retun: post details from the detail page\n",
    "\n",
    "    print('Scraping ',job_href,'...')\n",
    "    baselink = 'https://www.jobstreet.com.sg'\n",
    "   \n",
    "    r = requests.get(baselink+job_href.get('href'))\n",
    "    \n",
    "    if r.status_code == 429:\n",
    "        time.sleep(int(r.headers[\"Retry-After\"]))\n",
    "        r = requests.get(baselink+job_href.get('href'))\n",
    "        \n",
    "    soup = BeautifulSoup(r.content,'html.parser')\n",
    "    \n",
    "    dict={}\n",
    "    dict['company_name'] = soup.find_all('div',{'class':'FYwKg _6Gmbl_0'})[1].text.strip() if soup.find('div',{'class':'FYwKg _6Gmbl_0'}) else None\n",
    "    dict['job_title'] = soup.find_all('div',{'class':'FYwKg _6Gmbl_0'})[2].text.strip() if soup.find('div',{'class':'FYwKg _6Gmbl_0'}) else None\n",
    "    for i in np.arange(4,len(soup.find_all('div',{'class':\"FYwKg zoxBO_0\"})),2):\n",
    "        try:\n",
    "            dict[soup.find_all('div',{'class':\"FYwKg zoxBO_0\"})[i].text]=soup.find_all('div',{'class':\"FYwKg zoxBO_0\"})[i+1].text  \n",
    "        except IndexError:\n",
    "            dict[soup.find_all('div',{'class':\"FYwKg zoxBO_0\"})[i].text] = ''\n",
    "    dict['location'] = soup.find_all('div',{'class':\"FYwKg _11hx2_0\"})[0].text.strip() if soup.find('div',{'class':'FYwKg _11hx2_0'}) else None\n",
    "    \n",
    "    dict['salary'] = soup.find_all('div',{'class':\"FYwKg _11hx2_0\"})[1].text.strip().replace(u'\\xa0', u'') if len(soup.find_all('div',{'class':\"FYwKg _11hx2_0\"}))>2 else None\n",
    "    dict['date_posted'] = soup.find_all('div',{'class':\"FYwKg _11hx2_0\"})[-1].text.strip() if soup.find('div',{'class':\"FYwKg _11hx2_0\"}) else None\n",
    "    \n",
    "    return [dict['company_name'],dict['job_title'],dict.get('Industry',None),dict.get('Qualification',None),dict.get('location',None),dict.get('salary',None),dict.get('date_posted',None)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##================================================Generic Function================================================##\n",
    "\n",
    "def save_jobs_to_excel(jobs_list,filename):\n",
    "    if isinstance(jobs_list,dict):\n",
    "        jobs = pd.DataFrame(jobs_list)\n",
    "        jobs.to_excel(filename)\n",
    "    if isinstance(jobs_list,pd.DataFrame):\n",
    "        jobs_list.to_excel(filename)\n",
    " \n",
    "    \n",
    "##=================================================Main Function=================================================##\n",
    "\n",
    "def find_jobs_from(website, job_title, location, desired_characs, filename = 'results.xlsx'):\n",
    "    '''\n",
    "    This function extracs all the desired charateristics fo all new job postings of the title and location specified and return\n",
    "    them in a single file.\n",
    "    The arguments it take are:\n",
    "    -website: to specify ('Indeed' or 'JobsDB')\n",
    "    -Job_title\n",
    "    -Location\n",
    "    -Desired characs: list of characteristics to be returned\n",
    "    -Filename: to specify the filename and format of the output\n",
    "    '''\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        job_soup = load_indeed_jobs_div(job_title,location)\n",
    "        jobs_list, num_listings = extract_job_information_indeed(job_soup,desired_characs)\n",
    "        \n",
    "    if website == 'Jobstreet':\n",
    "        s = requests.session()\n",
    "        links_dict = linksByRoles(job_title)\n",
    "        jobs_list, num_listings = parseLinks(links_dict)\n",
    "   \n",
    "        \n",
    "    save_jobs_to_excel(jobs_list,filename)\n",
    "    \n",
    "    print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings,website,filename))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_characs = ['titles','companies','links','date_listed','salary']\n",
    "\n",
    "find_jobs_from('Jobstreet',['ite graduate'],'Singapore', desired_characs,filename = 'jobstreet.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Programme"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
